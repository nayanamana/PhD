#!/usr/local/bin/python3.8

# Author:   Samuel Marchal samuel.marchal@aalto.fi
# Copyright 2015 Secure Systems Group, Aalto University, https://se-sy.org/
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#     http://www.apache.org/licenses/LICENSE-2.0
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import statistics
import math
import time
from urllib import parse
import re
import sys
import json

sys.path.append(os.path.dirname(os.path.realpath(__file__)) + '/lib')

from unidecode import unidecode
#from website import Website
from extract_URL import Extractor
import pandas as pd
import datetime
import glob
import traceback

from website import Website

def cleanString(s):
    s = unidecode(s)
    s = re.sub('\n',' ',s)
    s = re.sub('\t',' ',s)
    return str.lower(s)

def cleanURL(s):    # clean the domain names obtained to respect the DNS rules
    if s[:2] == "b'":
        s = s[2:len(s)-1]
    s = parse.unquote(s)
    return str.lower(unidecode(s))

def mergeset(l1,l2):
    if l2 != None:
        for element in l2:
            if len(element) > 2:
                if element in l1:
                    l1[element] += 1
                else:
                    l1[element] = 1 

def merge(l1,l2):
    for element,count in l2.items():
        if element in l1:
           l1[element] += 1
        else:
           l1[element] = 1 



def loadBrandList(thefile):

    f = open(thefile,'r')
    brands = f.readlines()
    return [x.strip() for x in brands]



def jaccard2(dic1, dic2):

    if len(dic1) == 0 or len(dic2) == 0:
        return 0

    count_inter = 0
    count_comp = 0
    for element,count in dic1.items():
        if element in dic2:
           count_inter += dic2[element] + count
        else:
           count_comp += count

    for element,count in dic2.items():
        if element not in dic1:
           count_comp += count

    return float(count_inter) / float(count_inter+count_comp)


def jaccard(dic1, dic2): #hellinger distance
    
    if len(dic1) == 0 or len(dic2) == 0:
        return 1.0
    
    h2 = 0
    count_dic1 = float(sum(dic1.values()))
    count_dic2 = float(sum(dic2.values()))


    for element,count in dic1.items():
        if element in dic2:
            h2 += math.pow(math.sqrt(float(dic2[element])/count_dic2) - math.sqrt(float(count)/count_dic1),2)
        else:
            h2 += float(count)/count_dic1

    for element,count in dic2.items():
        if element not in dic1:
            h2 += float(dic2[element])/count_dic2

    return h2/2


def fill_empty(vect):
    if vect == []:
        return [0,0]
    elif len(vect) == 1:
        return [vect[0],vect[0]]
    else:
        return vect



IP_pat = re.compile("\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}")
brands = loadBrandList("./data/phish_brand.txt")

################### main function

def feature_vector(extractor,ws):


    ############## variables declaration
    proto_phish = []
    levels_phish = []
    url_len_phish = []
    domain_len_phish = []
    mld_len_phish = []
    word_mld_phish = []
    word_len_phish = []

    phishers_mld = set()
    phish_mldlist = {}

    offset = 0
    feat_vect = dict((el,0) for el in range(offset + 60))
    #feat_vect = {}

    ############### features extraction for starting URL feat[118-125]
    starturl = cleanURL(ws.starturl)
    domain,levels,mld,word,mldlist = extractor.extract_words(starturl)
    start_word = word
    start_mldlist = mldlist
    start_mld = "=="
    phishers_mld.add(mld)

    if mldlist is not None:
        start_mld = mld
        if len(start_mldlist) > 1:
            start_mldlist.append(mld)
        mergeset(phish_mldlist,start_mldlist)
        feat_vect[0] = extractor.proto # https connection
        feat_vect[1] = levels # count of level domain
        feat_vect[2] = len(starturl) # url character length
        feat_vect[3] = len(domain) # DN character length
        feat_vect[4] = len(mld) # mld character length
        feat_vect[5] = len(mldlist) # count of labels in mld
        feat_vect[6] = len(word) # count of labels in URL
        #feat_vect[offset+7] = len(starturl.split('.')) - levels


    #################### features extraction for landing URL feat[126-133]
    landurl = cleanURL(ws.landurl)
    domain,levels,mld,word,mldlist = extractor.extract_words(landurl)
    land_word = word
    land_mldlist = mldlist
    land_mld = "=="


    if mld not in phishers_mld:
        phishers_mld.add(mld)
        feat_vect[offset] = 0 #starting = landing mld
    else:
        feat_vect[offset] = 1

    if mldlist is not None:
        land_mld = mld
        if len(land_mldlist) > 1:
            land_mldlist.append(mld)
        mergeset(phish_mldlist,land_mldlist)
        feat_vect[7] = extractor.proto
        feat_vect[8] = levels
        feat_vect[9] = len(landurl)
        feat_vect[10] = len(domain)
        feat_vect[11] = len(mld)
        feat_vect[12] = len(mldlist)
        feat_vect[13] = len(word)
        #feat_vect[offset+7] = len(landurl.split('.')) - levels

    ##################### redirection related features
    for url in ws.redirections[1:]:
        domain,levels,mld,word,mldlist = extractor.extract_words(cleanURL(url))
        phishers_mld.add(mld)

        if mldlist != None:
            if len(mldlist) > 1:
                mldlist.append(mld)
            mergeset(phish_mldlist,mldlist)


    # href features
    href = ws.source_links

    feat_vect[14] = len(ws.redirections) # count of redirections
    feat_vect[15] = len(ws.redirections) - len(phishers_mld) + 1 # count of redirections with no changing DN
    feat_vect[16] = len(phishers_mld) + 1 # count of internal DN
    feat_vect[17] = len(href) # count of href

    ##################### text content features
    text_words = []
    title_words = []

    for element in re.split("\s+",ws.text_without_title):
        record = 1
        tokens = re.split("[^a-z]+",cleanString(element))
        for token in tokens:
            if len(token) == 0:
                record = 0
            if len(token) > 2:
                text_words.append(token)
        if record and len(tokens)>1:
            text_words.append(cleanString(element))

    for element in ws.title.split(" "):
        record = 1
        tokens = re.split("[^a-z]+",cleanString(element))
        for token in tokens:
            if len(token) == 0:
                record = 0
            if len(token) > 2:
                title_words.append(token)
        if record and len(tokens)>1:
            title_words.append(cleanString(element))

    feat_vect[18] = ws.input_count
    feat_vect[19] = ws.image_count


    ######### relatedness features
    text_words_dic = {}
    mergeset(text_words_dic,text_words)
    title_words_dic = {}
    mergeset(title_words_dic,title_words)
    land_mlddic = {}
    mergeset(land_mlddic,land_mldlist)
    start_mlddic = {}
    mergeset(start_mlddic,start_mldlist)

    feat_vect[20] = len(text_words)
    feat_vect[21] = len(title_words)


    feat_vect[22] = jaccard(title_words_dic, land_word)
    feat_vect[23] = jaccard(title_words_dic, start_word)
    feat_vect[24] = jaccard(title_words_dic, land_mlddic) # good indicator ?
    feat_vect[25] = jaccard(title_words_dic, start_mlddic) # good indicator ?
    feat_vect[26] = jaccard(title_words_dic, phish_mldlist) # good indicator ?
    feat_vect[27] = jaccard(title_words_dic, text_words_dic)

    feat_vect[28] = jaccard(text_words_dic, land_word)
    feat_vect[29] = jaccard(text_words_dic, start_word)
    feat_vect[30] = jaccard(text_words_dic, land_mlddic)
    feat_vect[31] = jaccard(text_words_dic, start_mlddic)
    feat_vect[32] = jaccard(text_words_dic, phish_mldlist) # good indicator ? yes

    feat_vect[33] = jaccard(start_mlddic,land_word)
    feat_vect[34] = jaccard(start_mlddic,start_word)
    feat_vect[35] = jaccard(start_mlddic,land_mlddic)
    feat_vect[36] = jaccard(start_mlddic,phish_mldlist)

    feat_vect[37] = jaccard(land_mlddic,land_word)
    feat_vect[38] = jaccard(land_mlddic,start_word)
    feat_vect[39] = jaccard(land_mlddic,phish_mldlist)

    feat_vect[40] = jaccard(phish_mldlist,land_word)
    feat_vect[41] = jaccard(phish_mldlist,start_word)

    feat_vect[42] = jaccard(start_word, land_word)


    if start_mld in title_words_dic:
        feat_vect[43] = 1
    if start_mld in text_words_dic:
        feat_vect[44] = 1

    if land_mld in title_words_dic:
        feat_vect[45] = 1
    if land_mld in text_words_dic:
        feat_vect[46] = 1

    feat_vect[47] = ws.external_source #external sources of code

    '''
    if feat_vect[21] > 0.:
        feat_vect[96] = 1
    if feat_vect[22] > 0.:
        feat_vect[97] = 1
    '''

    # title words in starting and landing URL
    count_occ = 0
    starturlcount = 0
    startmldcount = 0
    landurlcount = 0
    landmldcount = 0
    for label, occ in title_words_dic.items():
        count_occ += occ
        if start_mld.find(label) < 0:
            if starturl.find(label) > -1:
                starturlcount += occ
        else:
            startmldcount += occ

        if land_mld.find(label) < 0:
            if landurl.find(label) > -1:
                landurlcount += occ
        else:
            landmldcount += occ

    if count_occ != 0:
        feat_vect[48] = math.sqrt(float(starturlcount)) / math.sqrt(float(count_occ))
        feat_vect[49] = math.sqrt(float(startmldcount)) / math.sqrt(float(count_occ))
        feat_vect[50] = math.sqrt(float(landurlcount)) / math.sqrt(float(count_occ))
        feat_vect[51] = math.sqrt(float(landmldcount)) / math.sqrt(float(count_occ))


    # text words in starting and landing URL
    count_occ = 0
    starturlcount = 0
    startmldcount = 0
    landurlcount = 0
    landmldcount = 0
    for label, occ in text_words_dic.items():
        count_occ += occ
        if start_mld.find(label) < 0:
            if starturl.find(label) > -1:
                starturlcount += occ
        else:
            startmldcount += occ

        if land_mld.find(label) < 0:
            if landurl.find(label) > -1:
                landurlcount += occ
        else:
            landmldcount += occ

    if count_occ != 0:
        feat_vect[52] = math.sqrt(float(starturlcount)) / math.sqrt(float(count_occ))
        feat_vect[53] = math.sqrt(float(startmldcount)) / math.sqrt(float(count_occ))
        feat_vect[54] = math.sqrt(float(landurlcount)) / math.sqrt(float(count_occ))
        feat_vect[55] = math.sqrt(float(landmldcount)) / math.sqrt(float(count_occ))


    ######## new brand checking features in staring and landing MLD
    for brand in brands:

        if starturl.find(brand) > -1:
            if start_mld.find(brand) > -1 and len(start_mld) > len(brand):
                feat_vect[56] = 1
            elif start_mld.find(brand) < 0:
                feat_vect[57] = 1

        if landurl.find(brand) > -1:
            if land_mld.find(brand) > -1 and len(land_mld) > len(brand):
                feat_vect[58] = 1
            elif land_mld.find(brand) < 0:
                feat_vect[59] = 1

    return feat_vect

def clean_string(s):
    s = unidecode(s)
    s = re.sub('\n',' ',s)
    s = re.sub('\t',' ',s)
    return str.lower(s)

def build_feature_vector(dirname, model_path):
   sys.setrecursionlimit(10000)
   feat_vec_dict = {}
   feat_vec_dict_temp = {}

   no_phish = 0
   no_ben = 0
   no_proc = 0

   #print(brands)

   for fl in glob.glob(dirname + '/*.json'):
      #print(fl)
      extractor = Extractor()

      text = ""
      try:
         with open(fl, encoding="utf8") as fi:
             text = fi.read()
         text = clean_string(text)
         #js = json.loads(text)
         ws = Website(jspath=fl)

         #print(js)
         feat_vect_site = feature_vector(extractor,ws)
         #print(ws.is_phish)
         if ws.is_phish:
            no_phish += 1
         else:
            no_ben+= 1

         feat_vec_dict_temp[ws.starturl] = {'is_phish': ws.is_phish, 'data': feat_vect_site}
         
      except Exception as e:
         print(str(e))
         traceback.print_exc(file=sys.stdout)

   if no_phish > no_ben:
      no_proc = no_ben
   else:
      no_proc = no_phish

   no_tmp_p = 0
   no_tmp_b = 0

   j = 0
   for item in feat_vec_dict_temp:
      if feat_vec_dict_temp[item]['is_phish']:
         no_tmp_p += 1
         if no_tmp_p <= no_proc:
            data = feat_vec_dict_temp[item]['data']
            data['start_url'] = item
            data['label'] = feat_vec_dict_temp[item]['is_phish']
            feat_vec_dict[j] = data
            j += 1
      else:
         no_tmp_b += 1
         if no_tmp_b <= no_proc:
            data = feat_vec_dict_temp[item]['data']
            data['start_url'] = item
            data['label'] = feat_vec_dict_temp[item]['is_phish']
            feat_vec_dict[j] = data
            j += 1

   #return feat_vec_dict
   featvecmat = pd.DataFrame(feat_vec_dict)
   print(featvecmat.shape)
   featvecmat.transpose().to_pickle(model_path)


current_milli_time = lambda: int(round(time.time() * 1000))

if __name__=="__main__":
    noop

    '''
    if len(sys.argv) != 4:
        print("usage: build_feat_vec.py webiste_dir prefix label(0:leg/1:phish)")
    else:

        sys.setrecursionlimit(10000)
        websitedir = os.path.abspath(sys.argv[1])
        extractor = Extractor()
        label = int(sys.argv[3])
        feat_vec_temp = {}
        print(brands)



        i = 0
        
        pd.set_option('display.max_rows', 1000)
        
        #time_stats = open("timestats2.csv",'w', encoding="utf8")


        
        for f in sorted(os.listdir(websitedir)):
            start_time = current_milli_time()

            if f.find(".json") > 0:
             print(websitedir + "/" + f)
             ws = Website(jspath=websitedir + "/" + f)
             intermediate = current_milli_time()
             feat_vect_site = feature_vector(extractor,ws)
             end_time = current_milli_time()
             #time_stats.write(str(intermediate-start_time) + "," + str(end_time-intermediate) + "\n")
             feat_vect_site["start_url"] = f
             feat_vect_site["label"] = label
             feat_vec_temp[i] = feat_vect_site
             i += 1
             print(ws.starturl)

            elif f.find(".png") < 0:
             ws = Website(websitedir + "/" + f + "/sitedata.json")
             intermediate = current_milli_time()
             feat_vect_site = feature_vector(extractor,ws)
             end_time = current_milli_time()
             #time_stats.write(str(intermediate-start_time) + "," + str(end_time-intermediate) + "\n")
             feat_vect_site["start_url"] = ws.starturl
             feat_vect_site["label"] = label
             feat_vec_temp[i] = feat_vect_site
             i += 1
             print(ws.starturl)

        #time_stats.close()
        featvecmat = pd.DataFrame(feat_vec_temp)
        print(featvecmat.shape)
        featvecmat.transpose().to_pickle(sys.argv[2] + "_fvm.pkl")
    '''
